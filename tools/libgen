#!/usr/bin/python3

import requests
from bs4 import BeautifulSoup
from pprint import pprint
from urllib.parse import urljoin


def soup_scrape():
    headers = {
    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
              }
    s_url =  "https://libgen.is/search.php?req="
    p_url = "https://libgen.is/"
    search_term = input("Search Term: ")
    headers = {
        'accept-language': 'en-US,en;q=0.9,fr-FR;q=0.8,fr;q=0.7,ar;q=0.6',
        'user-agent': 'Firefox'                                                
    }
    page = requests.get(s_url + search_term, headers=headers)
    soup = BeautifulSoup(page.content, "html.parser")
    table_rows = soup.find_all('tr')
    for tr in table_rows:
        '''
        for t_ind, td in enumerate(tr.descendants):
            print(t_ind, "#"*10)
            print(td)
        '''
        table_data = tr.find_all('td')
        for t_ind, td in enumerate(table_data):
            a_elts = td.find_all('a')
            for tt_ind, ttd in enumerate(a_elts):
                link = ttd['href']
                if "book" in link:
                    print(ttd.get_text())
                    print(p_url + link)
                    print('\n')
                #if "book" in ttd:
                #    print(ttd)
        #print(tr.prettify())
    #print(soup.prettify())

soup_scrape()
