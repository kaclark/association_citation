<html><head>
    <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
    <title>Markov_Chains</title>
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../include/main.css"/>
    </head>
    <body>
    <div class="content">
        <h1>Markov Chains</h1>
        <p></p>
    </div>
    <div class="content" id="content">
    
    <p>Let Xi ∈ {A, . . . , E} denote the random variable that corresponds to X’s location after the ith move. Let X0 = A. From before, Pr[X1 = A] = 0 and Pr[X1 = L] = 1 4 for L ∈ {B, . . . , E}. We have Pr[X2 = A] = ∑ L∈{B,...,E} Pr[X2 = A|X1 = L] Pr[X1 = L] = 1 4 ∑ L∈{B,...,E} Pr[X2 = A|X1 = L] = 1 4 ∑ L∈{B,...,E} 1 4 = 1 pg 2</p>
    
    <p>WSY =       0 1 4 1 4 1 4 1 4 1 4 0 1 4 1 4 1 4 1 4 1 4 0 1 4 1 4 1 4 1 4 1 4 0 1 4 1 4 1 4 1 4 1 4 0 pg 3</p>
    
    <p>n its simplest sense, a discrete-time random or stochastic process is a collection of random variables indexed by time. The random variables track the pertinent aspects of the state pg 8</p>
    
    <p>of the system undergoing the process that we are trying to analyze. We denote such a process as {X(t) : t ∈ N} or {Xt : t ∈ N} pg 9</p>
    
    <p>Markov process is a memoryless6 stochastic process. Informally, this means that future outcomes of the process are based solely on the current state and hence are just as deter- mined as when given the process’s full history leading up to the current state. pg 10</p>
    
    <p>As we have already seen in a sense, we can depict such Markov processes graphically. We call such depictions Markov chains. We will now define this notion formally. Let X = {Xt : t ∈ N} be a Markov process, where Xt ∈ D for all t ∈ N. We define the Markov Chains for X as the set of weighted directed graphs GX ,t = {GX ,t = (D, Et, wt)} where Et = {(u, v) : Pr[Xt+1 = v|Xt = u] > 0} and wt(u, v) = Pr[Xt+1 = v|Xt = u] pg 10</p>
    
    <p>In an abstract sense, a random walk is a random process that describes a path that consists  of a succession of random steps on some mathematical space. In our case, the mathemati-  cal space will be the vertices of a graph. Let G = (V, E, w) be a weighted graph. A random  walk on G is a sequence of steps along the edges of G where in any step, the probability  of taking the edge (u, v) while at node u is w(u, v)7. Random walks can be defined on any  graph, including undirected or unweighted ones. For undirected graphs, we assume each  edge is bidirectional, and for unweighted graphs, the probability of moving from a vertex  to its neighbor is the inverse of its (outgoing) degree.  pg 11</p>
    
    <p>Strongly Connected. A directed (undirected) graph is said to be strongly connected (con-  nected) or irreducible if there exists a directed (undirected) path connecting every ordered (un-  ordered) pair of vertices.  For an undirected graph, being connected and strongly connected are identical. In gen-  eral, any undirected graph can be seen as a collection of connected components. If there is  only one connected component, the graph is said to be (strongly) connected or irreducible.  The situation is a little more complicated for directed graphs. Directed graphs can be  decomposed into strongly connected components, or what we will call communicating  classes. Let us say that two vertices u and v in a directed graph G communicate with one  another if and only there is a directed path in G from u to v and from v to u. This relation  between vertices is actually an equivalence relation as it is:  • Reflexive: There is a path of length 0 from any vertex to itself.  • Symmetric: By definition, there is a directed path in G from u to v and from v to u.  • Transitive: If u and v communicate, and v and w, we can stitch the corresponding  paths together to conclude that u and w do too.  Therefore, the relation partitions the vertices of any directed graph into equivalence classes  which we call communicating classes. By definition, a communicating class is maximally  strongly connected and is hence a strongly connected component. The crucial difference  between the directed and undirected case is that there may be edges between strongly  connected components in directed graphs, but there cannot be any edges between con-  nected components in undirected graphs. pg 15</p>
    
    <p>Theorem (Fundamental Theorem of Markov chains). Let X = {Xt : t ∈ N} be a time-  homogenous Markov process, where Xt ∈ D for all t ∈ N and GX be its Markov chain. If GX is  strongly connected and aperiodic, then, every random walk on GX converges to a unique  stationary distribution. pg 17</p>
    
    <p>Finally, here is an observation on random walks in undirected graphs. The unique station-  ary distribution of a connected undirected graph is the uniform distribution normalized  by its degree sequence pg 19</p>
    
    <p>In undirected graphs, one often considers a variant of random walks called lazy random  walks. In an α-lazy random walk, at any time step, we either stay at the current vertex  with probability α, or move to random neighbor with probability 1−α. The random walk  we have been looking at thus far is the 0-lazy random walk. Typically lazy random walks  set α =  1  2 . In Scotland Yard v0, we were effectively looking at a 1  5 -lazy random walk.  Notice that a lazy random walk implicitly makes the graph non-bipartite and aperiodic.  This means that every α-lazy random walk on a connected undirected graph converges  to its unique stationary distribution if α > 0. This forced convergence is the reason lazy  random walks are often considered important in practice. pg</p>
    
    </div>
    </body>
    </html>
    